# Machine Learning 
## week 1 
* Introduction to Machine Learning 
* Setup for scientific Python environment
## week 2
* Review on Linear Algebra
    * Span & Linear Dependence | Norms | Eigendecomposition | Singular Value Decomposition | Traces | Determinant
* Data Exploration & PCA
    * This lab works through the process of **Exploratory Data Analysis (EDA)** and discuss how to leverage the **Principle Component Analysis (PCA)** to *visualize* and understand *high-dimensional data*.
    
    [Lab 2](https://github.com/YenTingWangTW/Machine-Learning/blob/master/week_2/Lab02.ipynb)

## week 3
* Review on Probability & Information Theory
    * Random Variables & Probability Distributions | Multivariate & Derived Random Variables | Bayesâ€™ Rule & Statistics | Principal Components Analysis | Information Theory | Decision Trees & Random Forest
* Decision Trees & Random Forest
    * This lab applies the **Decision Tree** and **Random Forest algorithms** to the *classification* and *dimension reduction* problems using the Wine dataset.
    
    [Lab 3](https://github.com/YenTingWangTW/Machine-Learning/blob/master/week_3/Lab03.ipynb)

## week 4
* Numerical Optimization
    * Numerical Computation | Optimization Problems | Unconstrained Optimization | Stochastic Gradient Descent | Perceptron | Adaline | Constrained Optimization | Linear & Polynomial Regression | Duality
* 4-1 Perceptron & Adaline
    * This lab works through the implementation of **Perceptron** and **Adaline**, two of the first machine learning algorithms for the classification problem and trains these models using the **Optimization** techniques.

    [Lab 4-1](https://github.com/YenTingWangTW/Machine-Learning/blob/master/week_4/Lab04-1.ipynb)
* 4-2 Regression
    * This lab applies the **Linear and Polynomial Regression** using the Housing dataset. We will also extend the **Decision Tree** and **Random Forest classifiers** to solve the regression problem.

    [Lab 4-2](https://github.com/YenTingWangTW/Machine-Learning/blob/master/week_4/Lab04-2.ipynb)

## week 5
* Learning Theory & Regularization
    * Point Estimation | Bias & Variance | Consistency | Decomposing Generalization Error | Weight Decay | Validation
* Regularization
    * This lab works through some common **Regularization Techniques** such as *weight decay*, *sparse weight*, and *validation*.

    [Lab 5](https://github.com/YenTingWangTW/Machine-Learning/blob/master/week_5/Lab05.ipynb)

## week 6
* Probabilistic Models
    * Maximum Likelihood Estimation | Maximum A Posteriori Estimation | Bayesian Estimation
* Logistic Regression & Metrics
    * This lab practices **Logistic Regression** and applies some common evaluation metrics such as **Confusion Matrix**.

    [Lab 6](https://github.com/YenTingWangTW/Machine-Learning/blob/master/week_6/Lab06.ipynb)

## week 7
* Non-Parametric Methods & SVMs
    * KNNs | Parzen Windows | Local Models | Support Vector Classification (SVC) | Nonlinear SVC | Kernel Trick
* SVMs & Scikit-Learn Pipelines
    * This lab classifies *nonlinearly separable data* using the **KNN** and **SVM** classifiers and packs multiple data preprocessing steps into a *single Pipeline* in Scikit-learn to simplify the training workflow.

    [Lab 7](https://github.com/YenTingWangTW/Machine-Learning/blob/master/week_7/Lab07.ipynb)

## week 8
* Cross Validation & Ensembling
    * CV | How Many Folds? | Voting | Bagging | Boosting | Why AdaBoost Works?
* CV & Ensembling
    * This lab works through the **Cross Validation** technique for *hyperparameter selection* and practices some *ensemble learning* techniques.

    [Lab 8](https://github.com/YenTingWangTW/Machine-Learning/blob/master/week_8/Lab08.ipynb)

## week 10
* Neural Networks: Design
    * NN Basics | Learning the XOR | Back Propagation | Cost Function & Output Neurons | Hidden Neurons | Architecture Design
* TensorFlow101 & Word2Vec
    * This lab works through basic concepts of **TensorFlow** and trains a simple neural network with MNIST dataset.

    [Lab 10](https://github.com/YenTingWangTW/Machine-Learning/blob/master/week__10/Lab10.ipynb)

## week 12
* Convolutional Neural Networks
    * Convolution Layers | Pooling Layers | Variants & Case Studies | Visualizing Activations | Visualizing Filters/Kernels | Visualizing Gradients | Dreaming and Style Transfer | Segmentation and Localization | Object Detection | More Applications
* Nuts and Bolts of Convolutional Neural Networks
    * This lab works through two datasets, *MNIST* and *CIFAR-10* and implements CNN models for these two datasets using tensorflow. 

    [Lab 12](https://github.com/YenTingWangTW/Machine-Learning/blob/master/week__12/Lab12.ipynb)
    
